[{"content":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/M1ve4sudqlI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003ePatients in an unconscious state often encounter hydration issues, leading to dryness in their lips and the interior of their mouths. Given their inability to consume water independently, administering regular quantities of water poses a risk. Caregivers typically resort to using a moisturized cotton swab to gently moisten the patient\u0026rsquo;s lips, which serves the dual purpose of hydrating the area and assisting with oral cleanliness, as well as facilitating the practice of swallowing functions. However, this method demands continuous, repetitive effort from caregivers, potentially around the clock.\u003c/p\u003e\n\u003cp\u003eThe introduction of a robotic solution could address this challenge efficiently, allowing healthcare professionals and caregivers to allocate their time to other critical tasks.\u003c/p\u003e\n\u003cp\u003eThe project\u0026rsquo;s goal is to enable a robot to detect lip landmarks and accurately apply a cotton swab to the lips.\u003c/p\u003e\n\u003ch2 id=\"overall-system\"\u003eOverall System\u003c/h2\u003e\n\u003cp\u003eThe image below shows the different nodes in the system and how they communicate with each other. Each node\u0026rsquo;s funtionalities will be explained below.\u003c/p\u003e\n\u003cp\u003eThe project is mainly divided into two parts.\u003c/p\u003e\n\u003ch2 id=\"computer-vision\"\u003eComputer Vision\u003c/h2\u003e\n\u003cp\u003eThe computer vision component of the application harnesses the power of the dlib library, renowned for its deep learning to perform facial landmark detection by the shape predictor model, trained on an extensive dataset of facial landmarks. This model is adept at pinpointing the precise locations of significant points on the face, ensuring high accuracy in facial feature recognition.\u003c/p\u003e\n\u003cp\u003eFollowing the detection of facial landmarks, the 2D to 3D coordinate transformation is achieved by RealSense camera\u0026rsquo;s depth sensing. We integrate the Intel RealSense camera\u0026rsquo;s capabilities to align color and depth frame of image. This process involves capturing real-time video streams, processing the color images to identify facial landmarks, and then correlating these points with depth information from the aligned depth frame.\u003c/p\u003e\n\u003cp\u003eThe face_detection node will publish the 7 lip landmarks coordinates in an PoseArray to communicate with the MoveIt Node.\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-fourth-post/","title":"Robot doing Mouth Care for Unconscious Residents"},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthors: Ethan Parham, Jialu Yu, David Morris, Henry Meiselman\u003c/div\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/Intro_whale_1.webp\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eTagging whales enables us to characterize their health and lifestyle, allowing us as a species to adjust our actions and minimize our impact on these animals. Drone tagging proves superior to traditional boat tagging methods, as it reduces the time and costs associated with data collection and enables the tagging of smaller, more agile animals.\u003c/p\u003e\n\u003cp\u003eOne of the main challenges is the tags missing the whales; currently, the drone needs to be around 16 to 20 feet in the air to ensure the tag has the correct orientation to stick, negatively impacting drop accuracy. Another concern is whether prop wash affects the release of the tags.\u003c/p\u003e\n\u003cp\u003eTo address these issues, we will develop a \u003cstrong\u003ephysics-based model for dropping tags from drones\u003c/strong\u003e. This model will guide the design of the optimal release mechanism, ensuring the necessary impulse, orientation, and accuracy for successful attachment and target hits.\u003c/p\u003e\n\u003ch2 id=\"user-requirements-and-engineering-specifications\"\u003eUser Requirements and Engineering Specifications\u003c/h2\u003e\n\u003cp\u003eThe project requirements and specifications were formulated through discussions with the ocean research team and extensive research into considerations essential for a payload delivery system. These requirements are categorized into three aspects: physical requirements, performance requirements, and usage requirements. The user requirements and engineering specifications serve as the foundation for our design, guiding the development of an effective payload delivery system.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/user_spec.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"build-description\"\u003eBuild Description\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eDesign of Release Mechanism\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/release_design.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/DTAG.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe Figure above shows the CAD assembly of the release mechanism design and how it is installed onto the fin, clamp and the tag.\u003c/p\u003e\n\u003cp\u003eThe release mechanism is made up of four 3D printed components. The servo mount and spring mount connect to the drone, while the fin topper and fin topper disk connect to the top of the fins. The servo motor, mounted in the servo mount, drives a linkage which is used to pull the servo pin. When the pin is pulled, a spring mounted into the spring mount is uncompressed, which forces the tag assembly downwards. The tag assembly consists of the fin topper, fin topper disk, fin, clamp, and tag. The clamp and tag used in the testing are unmodified parts which were provided to us by the research team.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/release_design2.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/release_design3.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe two figures above illustrates the release mechanism before and after the pin is pulled. The compressed spring on the left image is not depicted for clarity. The spring is initially compressed, until the servo pin can be pushed into the catch on the fin topper. Then, when the pin is pulled the spring is uncompressed and the tag is launched downwards. The fin will be mounted to the bottom of the topper, in an assembly similar to the one shown below, and the additional initial velocity from releasing the energy of the spring allows the tag to travel faster than the drone downwash. As a result, we believe the tag will be more stable in flight since higher velocity air moving over the fins will create a larger corrective moment.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e4 New Fin Designs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/new_fin.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eWe additionally created four new fin designs to test in addition to the original design used. The new designs, shown above, are primarily variations on the original design which use the same head for mounting the DTAG clamp and hole for inserting the fin topper.\u003c/p\u003e\n\u003cp\u003eThe intent of the rotated fin was to move the fins into the cross-shaped region of less turbulent lower velocity air between the drone rotors. The trapezoidal fin shape mimics the shape of a dart fin, and the angled back may make the fin more stable at lower tag velocities when the downwash flows past the tag. The larger and smaller fin designs were created to test the effect changing the surface area of the fin had on the tag’s stability.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAll Components in a glance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/everything2.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eAll components engineered are visually represented in the graph below.\u003c/p\u003e\n\u003ch2 id=\"verification-plan\"\u003eVerification Plan\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003ePart I: Orientation Angle\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/rig1.png\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/rig2.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe orientation of the tag while falling through the air must remain less than 25° to meet our specification. We will verify this by using the mechanism to drop the tag from a test rig which is modeled in the figure below.\u003c/p\u003e\n\u003cp\u003eWe will record these drop tests and use the software Tracker to measure the maximum orientation angle of each drop. A screenshot of the Tracker software can be seen in Figure 32 below. The orientation angle over time is plotted in the top right corner of Figure 32. We can verify whether the orientation angle is less than 25° for each drop by looking at this plot.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePart II: Impact Force\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe impulsive impact force must be between 5 and 26.6 Ns in order to meet our specifications. We will verify this in a similar way as we verified the orientation angle, by using the Tracker software on a video recording of a physical test drop. The Tracker software in this case will measure the velocity of the tag during the descent. A screenshot of the Tracker software measuring velocity can be seen in figure below. We can plug the final velocity at impact into equation to determine the impulsive impact force based on this final velocity. From here we can verify whether the impulsive impact force of each drop test meets our specification of being in between 5 and 26.6 Ns.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/test_1.png\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eFin Chosen\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/test3.gif\" alt=\"targets\"\u003e\n\u003cimg src=\"/images/test4.gif\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003cp\u003eThe experiment results reveal that Fin 2, featuring the small fin, achieves the necessary impact forces at 12.4 N while keeping the tag orientation deviation within a favorable range of 3.2-10 degrees. This outcome underscores the effectiveness of our design in ensuring both adequate impact forces and minimal deviation in tag orientation. To visually showcase the performance of Fin 2, we have provided a GIF illustrating the test results for this specific fin design.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRelease Mechanism with Fin 2\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe accompanying image displays the final product, showcasing the release mechanism installed with the servo motor and Fin 2, reflecting the successful culmination of our efforts in creating a reliable and impactful drone tagging system.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/tag_whole.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-third-post/","title":"Drone Tagging"},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthor: Jialu Yu\u003c/div\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe focal point of this project is the modeling of the mechanical system of a jack bouncing around in the boundary of a box.\n\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/tao88HEVpCM\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\u003c/p\u003e\n\u003ch2 id=\"overall-system-and-reference-frames\"\u003eOverall System and Reference Frames\u003c/h2\u003e\n\u003cp\u003eAttached is a detailed drawing of the system, illustrating all the frames in use, complete with frame labels. These frames and their labels have been consistently used and identified in the code to maintain clarity and coherence.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/jack.jpeg\" alt=\"targets\"\u003e\n\u003cbr /\u003e\n\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFrames\u003c/strong\u003e\n\u003cbr /\u003e\nThe base frame for the box is called g_wf, where f is the center of the box.  \u003ccode\u003eg_f6, g_f7, g_f8, g_f9\u003c/code\u003e are the relative frames based on the box center. The frame between world and box vertices can be generated through cross product of \u003ccode\u003eg_wf\u003c/code\u003e and \u003ccode\u003eg_f6, g_f7, g_f8, g_f9\u003c/code\u003e.\nThe base frame for the jack is called g_wj, where j is the center of the jack.  \u003ccode\u003eg_jb,  g_ja, g_jc, g_jd\u003c/code\u003e are the relative frames based on the box center. The frame between world and box vertices can be generated through cross product of \u003ccode\u003eg_wj and g_jb,  g_ja, g_jc, g_jd\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eEuler-Lagrange equations\u003c/strong\u003e\n\u003cbr /\u003e\nThe Euler Lagrange equation is calculated based on the:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCalculate Lagrangian equation by \u003ccode\u003eL = KE - V\u003c/code\u003e, where \u003ccode\u003eKE\u003c/code\u003e is kinetic energy of both jack and box and \u003ccode\u003eV\u003c/code\u003e is the potential energy of both jack and box.\nThe python script calculating the kinetic energy \u003ccode\u003eKE\u003c/code\u003e and potential energy \u003ccode\u003eV\u003c/code\u003e is shown below:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e#Kinetic Energy and Potential Energy\n#frame kinetic energy\nKE_frame = sym.simplify(0.5*(v_wf).T*Inertia_frame_mat*v_wf)\n  #jack kinetic energy\nKE_vwa = sym.simplify(0.5*(v_wa).T*Inertia_jack_mat*v_wa)\nKE_vwb = sym.simplify(0.5*(v_wb).T*Inertia_jack_mat*v_wb)\nKE_vwc = sym.simplify(0.5*(v_wc).T*Inertia_jack_mat*v_wc)\nKE_vwd = sym.simplify(0.5*(v_wd).T*Inertia_jack_mat*v_wd)\nKE_jack = sym.simplify(KE_vwa + KE_vwb + KE_vwc + KE_vwd)\n\n\nKE_total = sym.simplify(KE_jack+ KE_frame)\n\n\nhf = sym.simplify(sym.Matrix([g_wf[1,3]]))\nh_wa = sym.simplify(sym.Matrix([g_wa[1,3]]))\nh_wb = sym.simplify(sym.Matrix([g_wb[1,3]]))\nh_wc = sym.simplify(sym.Matrix([g_wc[1,3]]))\nh_wd = sym.simplify(sym.Matrix([g_wd[1,3]]))\n\n\nV_f = Mf*gravity*hf[0]\nV_wa =  m_j_point*gravity*h_wa[0]\nV_wb =  m_j_point*gravity*h_wb[0]\nV_wc =  m_j_point*gravity*h_wc[0]\nV_wd =  m_j_point*gravity*h_wd[0]\n\n\nV_total = sym.simplify(sym.Matrix([V_f + V_wa + V_wb + V_wc + V_wd]))\n\u003c/code\u003e\u003c/pre\u003e\u003col start=\"2\"\u003e\n\u003cli\u003eDefine symtem variables as \u003ccode\u003eq = [x_f,  y_f, theta_f, x_j,  y_j,  theta_j]\u003c/code\u003e and use \u003ccode\u003eLagrangian Equation\u003c/code\u003e and \u003ccode\u003eq\u003c/code\u003e to calculate \u003ccode\u003eEuler Lagrangian equation\u003c/code\u003e.\u003cbr /\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c!-- $$\\frac{\\partial L}{\\partial \\dot{q}} \\bigg |^{\\tau +}_{\\tau -} = \\lambda \\frac{\\partial \\phi}{\\partial q^{'}}$$ --\u003e\n\u003cp\u003e\u003cstrong\u003eAdd the Constraints to the system\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo add the constraints to the system, constraints will be found through the area A between the vertex of the jack and the side of the box. The area could be found through the cross product of two vectors on side of the box and the jack vertex.\u003c/p\u003e\n\u003cp\u003eOnce the jack and the box collide, the area A should be 0. However, considering the movement tolerance, I set up a very small threshold for that.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eUpdate the external forces\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExternal forces are defined by a force matrix corresponding to the variables in q. I defined a torque of 30 N-m on the box and an upward force on the box to hold the box from falling down. The force matrix is \u003ccode\u003eForce_Matrix = symMatrix([0, Force, 30, 0, 0, jack_torque])\u003c/code\u003e.\nNote, the \u003ccode\u003ejack_torque = 0\u003c/code\u003e since they are point mass.\u003cbr /\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eApply the impact update laws\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eImpact update function can be found at cell 13 of 19 where it takes \u003ccode\u003ea current state variable\u003c/code\u003e and an index of \u003ccode\u003ephi, constraint\u003c/code\u003e. Then it will utilize the following function to do the impact update.\u003c/p\u003e\n\u003c!-- $$\\bigg [ \\frac{\\partial L}{\\partial \\dot{q}} \\cdot \\dot{q} - L(q, \\dot{q}) \\bigg] ^{\\tau +}_{\\tau -} = 0.$$ --\u003e","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-second-post/","title":"Simulating the Dynamics from Scratch-Modeling of a Jack Bouncing within the Box Boundaries."},{"content":"\u003cdiv style=\"text-align:center;\"\u003eAuthors: Joel Goh, Maximiliano Palay, Rahul Roy, Sophia Schiffer, Jialu Yu\u003c/div\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\n\u003cdiv style=\"position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;\"\u003e\n  \u003ciframe src=\"https://www.youtube.com/embed/HQIWntieInI\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;\" allowfullscreen title=\"YouTube Video\"\u003e\u003c/iframe\u003e\n\u003c/div\u003e\n\n\u003cp\u003eIn this project, the Franka arm searches for known randomly placed pins and shoots them down. The Franka arm is fitted with an onboard camera to search its environment and two Nerf blasters to be able to shoot the targets. There are two rounds of shooting, while during each rounds, the user can verbally specify colors of the pins that the arm is going to shoot and the Franka arm will pick up one gun to shoot all the designate colored pins at the environment.\u003c/p\u003e\n\u003ch2 id=\"overall-system\"\u003eOverall System\u003c/h2\u003e\n\u003cp\u003eThe image below shows the different nodes in the system and how they communicate with each other. Each node\u0026rsquo;s funtionalities will be explained below.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/bowling_nodes.jpg\" alt=\"targets\"\u003e\u003c/p\u003e\n\u003ch2 id=\"nodes\"\u003eNodes\u003c/h2\u003e\n\u003cp\u003eCrafted by our team within the ROS(Robot Operating System), this project features several key components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eControl Node\n\u003cul\u003e\n\u003cli\u003eActs as the main hub, coordinating tasks by calling services from other nodes to complete the demo.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMoveGun Node\n\u003cul\u003e\n\u003cli\u003eManages path planning and robot movement, handling everything from body to gripper. It uses MoveIt-interface services like Cartesian planning, IK planning, and gripper requests.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eYolo Node\n\u003cul\u003e\n\u003cli\u003eRuns YOLO (Real-Time Object Detection System) to find colored bowling pins in relation to the base of the Franka arm. Displays their positions as markers in Rviz2, a user-friendly graphical interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eUser_Input Node\n\u003cul\u003e\n\u003cli\u003eListens to the user\u0026rsquo;s audio input and commands the robot about the color of the targeted pins, making the system responsive to user instructions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTrigger\n\u003cul\u003e\n\u003cli\u003eControls the Arduino to automate the gun\u0026rsquo;s trigger, adding automation and precision to the project.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eApriltag_node\n\u003cul\u003e\n\u003cli\u003eCreated by the University of Michigan Team.\u003c/li\u003e\n\u003cli\u003eThis node scans and provides coordinates for April tags. We use it to help the robot grip the gun by accurately detecting its position. The AprilTag technology ensures precise and reliable positioning in our project.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"launch-files\"\u003eLaunch Files\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eShoot_pins.launch.xml in control package\n\u003cul\u003e\n\u003cli\u003eThis launch file starts up the six nodes listed above and RViz node to help visualize the target pins and robot movement simulation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"quickstart-to-reproduce-the-project\"\u003eQuickstart to reproduce the project\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eConnect the Arduino and RealSense camera to the computer\n\u003cul\u003e\n\u003cli\u003eOptional to connect an external microphone to your computer but the built-in computer microphone works as well\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEnsure that the two nerf guns are in front of the Franka arm roughly symmetrical around the y axis\n\u003cul\u003e\n\u003cli\u003eThe two guns need the apriltag 42 and 95 from the 36h11 family\u003c/li\u003e\n\u003cli\u003eEach gun is also loaded with 6 bullets\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStart the moveit group on the Franka station using \u003ccode\u003eros2 launch franka_moveit_config moveit.launch.py use_rviz:=false robot_ip:=panda0.robot\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eConnect to the realsense by running on your computer \u003ccode\u003eros2 launch realsense2_camera rs_launch.py depth_module.profile:=1280x720x30 pointcloud.enable:=true align_depth.enable:=true\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eOn your computer run the launch file, \u003ccode\u003eros2 launch control shoot_pins.launch.xml\u003c/code\u003e, which starts the rest of the necessary nodes\n\u003cul\u003e\n\u003cli\u003eAn error message will show with the msg, \u0026ldquo;Waiting for input\u0026rdquo;, where the audio input can be given starting the demo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n","description":null,"image":null,"permalink":"http://localhost:1313/posts/my-first-post/","title":"Franka Robot: Precision Pin-hunting Meets Playfulness"}]